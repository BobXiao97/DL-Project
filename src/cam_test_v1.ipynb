{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading label binarizer...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import joblib\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import cnn_models\n",
    "from gensim.corpora import Dictionary\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomCNN(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=29, bias=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "Model loaded\n",
      "frame\n",
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e273d133714a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# get the hand area on the video capture screen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m324\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m324\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m34\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mhand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhand_area\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-e273d133714a>\u001b[0m in \u001b[0;36mhand_area\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mhand_area\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mhand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m324\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m324\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mhand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mhand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# load label binarizer\n",
    "pred_text = \"\"\n",
    "lb = joblib.load('../outputs/lb.pkl')\n",
    "model = cnn_models.CustomCNN()\n",
    "model.load_state_dict(torch.load('../outputs/model.pth', map_location='cpu'))\n",
    "print(model)\n",
    "print('Model loaded')\n",
    "\n",
    "def hand_area(img):\n",
    "    hand = img[100:324, 100:324]\n",
    "    hand = cv2.resize(hand, (224,224))\n",
    "    return hand\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if (cap.isOpened() == False):\n",
    "    print('Error while trying to open camera. Plese check again...')\n",
    "# get the frame width and height\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "# define codec and create VideoWriter object\n",
    "out = cv2.VideoWriter('../outputs/asl.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 30, (frame_width,frame_height))\n",
    "\n",
    "# read until end of video\n",
    "while(cap.isOpened()):\n",
    "    # capture each frame of the video\n",
    "    ret, frame = cap.read()\n",
    "    print('frame')\n",
    "    print(frame)\n",
    "    # get the hand area on the video capture screen\n",
    "    cv2.rectangle(frame, (100, 100), (324, 324), (20, 34, 255), 2)\n",
    "    hand = hand_area(frame)\n",
    "    image = hand\n",
    "    \n",
    "    image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "    image = torch.tensor(image, dtype=torch.float)\n",
    "    image = image.unsqueeze(0)\n",
    "    \n",
    "    outputs = model(image)\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "    \n",
    "    cv2.putText(frame, lb.classes_[preds], (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "    \n",
    "    current_letter = ''\n",
    "    current_letter+=(lb.classes_[preds])\n",
    "    print(current_letter)\n",
    "    \n",
    "    pred_text = pred_text +\" \"+ lb.classes_[preds]\n",
    "#     print(lb.classes_[preds])\n",
    "    cv2.imshow('image', frame)\n",
    "    out.write(frame)\n",
    "    # press `q` to exit\n",
    "    if cv2.waitKey(27) & 0xFF == ord('q'):\n",
    "        break\n",
    "# release VideoCapture()\n",
    "cap.release()\n",
    "# close all frames and video windows\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"\\n\\n\\nPredicted Output:-\")\n",
    "print(pred_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KLVVWWWWWWWWWWWWWWKKWWWWWWWWWWWWWWWWWWWWWWWWWFWWWEEEEEEEEEEALKKKKKKKDKEECCCCCCAEAEEEEEEEEAAAAMAGAGGGGMNAAMANNMAAIIIKKKKKKKKKKKKKKKKDBDDDDDDDDDCLFFCCCDDDDDDDDCCCCDBDDDDDDDDDFFDDDFFFWFWWWWFEEEEEEEEEALAAAMNNBBBBNBBBBNNNNBBBBBBBBBBBCVRUXXXVREERRX'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = str(pred_text)\n",
    "t = t.replace(\"nothing\", \"\")\n",
    "t = t.replace(\"space\", \"\")\n",
    "t = t.replace(\"del\", \"\")\n",
    "t = t.replace(\" \", \"\")\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing........\n",
      "Predicted words from Video:- \n",
      "\n",
      "['Aanian' 'Aenian' 'Aerere' 'Aererin' 'Aererini' 'Aereris' 'Aerinin'\n",
      " 'Banian' 'Benian' 'Bereatin' 'Berele' 'Berere' 'Bereres' 'Bererin'\n",
      " 'Bererini' 'Berinin' 'Carelin' 'Celinin' 'Cereatin' 'Cereatini' 'Cerere'\n",
      " 'Cererin' 'Cerinin' 'Denian' 'Derelin' 'Derere' 'Dererin' 'Dererini'\n",
      " 'Dereris' 'Dorere' 'Eanerin' 'Eanialin' 'Eanian' 'Eenian' 'Eenier'\n",
      " 'Eerelin' 'Eererali' 'Eerere' 'Eererie' 'Eererin' 'Eererine' 'Eereris'\n",
      " 'Einerin' 'Faniani' 'Fereatin' 'Ferere' 'Fererin' 'Fererini' 'Ferinin'\n",
      " 'Foniali' 'Genian' 'Gerere' 'Gererin' 'Gerinin' 'Ienialin' 'Ieralin'\n",
      " 'Iereatin' 'Kaneatin' 'Kanian' 'Kelinin' 'Kenian' 'Kenier' 'Kereatin'\n",
      " 'Kereatis' 'Kerere' 'Kererin' 'Kererine' 'Kererini' 'Kerinin' 'Lanialin'\n",
      " 'Lererin' 'Lererini' 'Merere' 'Mererin' 'Nanian' 'Nelerin' 'Nerere'\n",
      " 'Nererin' 'Nereris' 'Rererin' 'Rerinin' 'Uanianin' 'Venian' 'Verealin'\n",
      " 'Vererin' 'Wanian' 'Wenialin' 'Wenian' 'Werelin' 'Werere' 'Wereria'\n",
      " 'Wererin' 'Wererini' 'Wererinis' 'Wereris' 'Wererisi' 'Werine' 'Werinin'\n",
      " 'Wonialin' 'Xanian' 'Xererin']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from preparing_data import all_categories\n",
    "from create_network import *\n",
    "from training import *\n",
    "\n",
    "output_name = \"\"\n",
    "out_name = []\n",
    " #Sample from a category and starting letter\n",
    "def sample(category, start_letter='A'):\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        category_tensor = categoryTensor(category)\n",
    "        input = inputTensor(start_letter)\n",
    "        hidden = rnn.initHidden()\n",
    "\n",
    "        output_name = start_letter\n",
    "\n",
    "        for i in range(max_length):\n",
    "            output, hidden = rnn(category_tensor, input[0], hidden)\n",
    "            topv, topi = output.topk(1)\n",
    "            topi = topi[0][0]\n",
    "            if topi == n_letters - 1:\n",
    "                break\n",
    "            else:\n",
    "                letter = all_letters[topi]\n",
    "                output_name += letter\n",
    "            input = inputTensor(letter)\n",
    "        out_name.append(output_name)\n",
    "\n",
    "\n",
    "# Get multiple samples from one category and multiple starting letters\n",
    "def samples(category, start_letters='ABC'):\n",
    "    for start_letter in start_letters:\n",
    "        sample(category, start_letter)\n",
    "print(\"Processing........\")\n",
    "for i in all_categories:\n",
    "    samples(i, t)\n",
    "\n",
    "import numpy as np\n",
    "print(\"Predicted words from Video:- \\n\")\n",
    "# function to get unique values\n",
    "def unique(list1):\n",
    "    x = np.array(list1)\n",
    "    print(np.unique(x))\n",
    "    \n",
    "unique(out_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
